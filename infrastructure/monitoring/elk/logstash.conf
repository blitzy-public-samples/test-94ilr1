# Logstash Configuration for AI-powered Email Management Platform
# Version: 8.10+
# Purpose: Production-grade log aggregation and processing with security and performance optimizations

#------------------------------------------------------------------------------
# Input Settings
#------------------------------------------------------------------------------
input {
  # Secure Filebeat input for application logs
  beats {
    port => 5044
    host => "0.0.0.0"
    ssl => true
    ssl_certificate => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
    ssl_verify_mode => "force_peer"
    include_codec_tag => true
    client_inactivity_timeout => 300
    id => "beats_input"
  }

  # Secure TCP input for system logs
  tcp {
    port => 5000
    type => "syslog"
    ssl_enable => true
    ssl_cert => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
    ssl_verify => true
    id => "tcp_input"
  }
}

#------------------------------------------------------------------------------
# Filter Settings
#------------------------------------------------------------------------------
filter {
  # Parse structured log messages
  grok {
    match => {
      "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} \[%{DATA:service}\] \[%{DATA:trace_id}\] %{GREEDYDATA:log_message}"
    }
    pattern_definitions => {
      "CUSTOM_TIMESTAMP" => "(?:%{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?)"
    }
    tag_on_failure => ["_grokparsefailure"]
    id => "main_log_parser"
  }

  # Enrich log data with additional fields
  mutate {
    add_field => {
      "environment" => "${ENV:production}"
      "datacenter" => "${DC_LOCATION}"
      "app_version" => "${APP_VERSION}"
    }
    convert => {
      "response_time" => "float"
      "status_code" => "integer"
    }
    remove_field => ["@version", "path"]
    id => "field_enrichment"
  }

  # Add performance metrics processing
  if [type] == "performance_metric" {
    ruby {
      code => '
        event.set("processing_time", 
          event.get("end_time").to_f - event.get("start_time").to_f
        )
      '
      id => "performance_calculator"
    }
  }

  # Add security event processing
  if [type] == "security_event" {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{WORD:security_level} %{IP:source_ip} %{WORD:action} %{GREEDYDATA:security_message}"
      }
      id => "security_event_parser"
    }
  }
}

#------------------------------------------------------------------------------
# Output Settings
#------------------------------------------------------------------------------
output {
  # Primary Elasticsearch output with security
  elasticsearch {
    hosts => ["${ES_HOSTS}"]
    user => "${ES_USER}"
    password => "${ES_PASSWORD}"
    ssl => true
    ssl_certificate_verification => true
    cacert => "/etc/logstash/certs/ca.crt"
    index => "email-platform-%{+YYYY.MM.dd}"
    template_name => "email-platform"
    template_overwrite => true
    ilm_enabled => true
    ilm_rollover_alias => "email-platform"
    ilm_pattern => "{now/d}-000001"
    pipeline => "email-platform-pipeline"
    id => "es_output_main"
  }

  # Dead letter queue for failed events
  if "_grokparsefailure" in [tags] {
    elasticsearch {
      hosts => ["${ES_HOSTS}"]
      user => "${ES_USER}"
      password => "${ES_PASSWORD}"
      ssl => true
      index => "logstash-dlq-%{+YYYY.MM.dd}"
      id => "es_output_dlq"
    }
  }
}

#------------------------------------------------------------------------------
# Pipeline Settings
#------------------------------------------------------------------------------
pipeline {
  workers => 4
  batch_size => 1000
  batch_delay => 50
  ordered => true
  safety_interval => 5
}

#------------------------------------------------------------------------------
# Queue Settings
#------------------------------------------------------------------------------
queue.type: persisted
queue.max_bytes: 1gb
queue.checkpoint.writes: 1000
queue.checkpoint.interval: "1m"
queue.page_capacity: "64mb"
dead_letter_queue.enable: true
dead_letter_queue.max_bytes: "1gb"

#------------------------------------------------------------------------------
# Monitoring Settings
#------------------------------------------------------------------------------
xpack.monitoring.enabled: true
xpack.monitoring.elasticsearch.hosts: ["http://elasticsearch:9200"]
xpack.monitoring.elasticsearch.username: "${MONITORING_USER}"
xpack.monitoring.elasticsearch.password: "${MONITORING_PASSWORD}"
xpack.monitoring.collection.interval: "10s"
xpack.monitoring.collection.pipeline.details.enabled: true

#------------------------------------------------------------------------------
# Performance Settings
#------------------------------------------------------------------------------
path.data: /var/lib/logstash
path.logs: /var/lib/logstash/logs
log.level: info
pipeline.workers: 4
pipeline.batch.size: 1000
pipeline.batch.delay: 50
queue.type: persisted
queue.max_bytes: "1gb"